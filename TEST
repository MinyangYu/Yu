# Urllib 基础
import urllib.request
urllib.request.urlretrieve(url,filename)#url 和 保存在本地地址
urllib.request.urlretrieve('http://www.hellobi.com', filename='D://1.html')
#用urlretrieve会产生一些缓存，可以用urlcleanup清楚
urllib.request.urlcleanup()

file = urllib.request.urlopen('http://www.hellobi.com')
file.info()
file.getcode()
file.geturl()

#超市设置
file = urllib.request.urlopen('http://www.hellobi.com', timeout=0.01)#会失败
file = urllib.request.urlopen('http://www.hellobi.com', timeout=1)#会成功
for i in range(0,100):
    try:
        file = urllib.request.urlopen('http://yum.iqianyue.com',timeout=1)
        data = file.read()
        print(len(data))
    except Exception as error:
        print(error)
        
#自动模拟HTTP请求
#有两重方式post或者get
'https://www.baidu.com/s?ie=utf-8&f=8'？号以后就是get的信息，等号前如：ie就是
字段名，等号后比如utf-8就是字段值
在百度搜索python，分析url，去掉一些没用的信息，简化后的url为
'https://www.baidu.com/s?wd=python'
keyword='python'
url='http://www.baidu.com/s?wd='+keyword
req=urllib.request.Request(url)
data=urllib.request.urlopen(req).read()
fh=open('D:/2.html','wb')
fh.write(data)
fh.close()
#如果是搜索中文，可以用quote去解决
keyword='俞珉洋'
keyword = urllib.request.quote(keyword)
url='http://www.baidu.com/s?wd='+keyword
req=urllib.request.Request(url)
data=urllib.request.urlopen(req).read()
fh=open('D:/3.html','wb')
fh.write(data)
fh.close()
import requests
url='http://www.baidu.com/s?wd='+keyword
r = requests.get(url)
fh=open('D:/3.html','wb')
fh.write(data)
fh.close()
print (r.status_code)

#post请求
import urllib.parse
url = 'https://www.iqianyue.com/mypost/'
data=urllib.parse.urlencode({
        'name':'wdasdwe',
        'pass':'dawdwds'}).encode('utf-8')
req=urllib.request.Request(url,data)
#req.add_header 可以伪装浏览器
data = urllib.request.urlopen(req).read()
fh=open('D:/3.html','wb')
fh.write(data)
fh.close() #成功

#爬虫的异常处理
'''
200   （成功）  服务器已成功处理了请求。 通常，这表示服务器提供了请求的网页。

400   （错误请求） 服务器不理解请求的语法。 
401   （未授权） 请求要求身份验证。 对于需要登录的网页，服务器可能返回此响应。 
403   （禁止） 服务器拒绝请求。
404   （未找到） 服务器找不到请求的网页。
405   （方法禁用） 禁用请求中指定的方法。 
407   （需要代理授权） 此状态代码与 401（未授权）类似，但指定请求者应当授权使用代理。
408   （请求超时）  服务器等候请求时发生超时。

500   （服务器内部错误）  服务器遇到错误，无法完成请求。 
503   （服务不可用） 服务器目前无法使用（由于超载或停机维护）。 通常，这只是暂时状态。 
505   （HTTP 版本不受支持） 服务器不支持请求中所用的 HTTP 协议版本。
'''
## URLError是父类，没有异常状态码；HTTPError是子类，有异常状态码与异常原因
##不能用URLError代替HTTPError
'''
URLError:
1.连不上服务器；
2.远程的触发不存在；
3.假如本地没有网络也会触发；
4.假如触发了HTTPError等子类。
'''
'''
import urllib.error
import urllib.request
try:
    data = urllib.request.urlopen('http://www.bilibili.com/')
except urllib.error.HTTPError as error:
    if hasattr(error,'code'):
        print(error.code)
    if hasattr(error,'reason'):
        print(error.reason)
403
Forbidden      
'''
#浏览器伪装
import urllib.request
url='https://blog.csdn.net/minyang_yu/article/details/103366505'
header = ('User-Agent','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36')
opener=urllib.request.build_opener()
opener.addheaders=[header]
data=opener.open(url).read()
fh=open('D:/4.html','wb')
fh.write(data)
fh.close() #乱码

#新闻爬虫需求及实现思路
#爬新浪新闻的首页，将所有新闻都下载到本地中
import urllib.request
import re
data = urllib.request.urlopen('https://news.sina.com.cn/').read()
data.decode('utf-8')#成功
data.decode('gbk')#失败
UnicodeDecodeError: 'gbk' codec can't decode byte 0xad in position 161: illegal multibyte sequence
data.decode('gbk','ignore')#成功但是乱码
data2= data.decode('utf-8')
